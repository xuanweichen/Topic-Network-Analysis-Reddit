{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "\n",
    "import pickle\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_posts, df_comments):\n",
    "    \n",
    "    # For posts data, replace any [removed], [deleted], NaN values with \"\" in the sub_body column then concatenate the sub_title and sub_body \n",
    "    df_posts.sub_body = df_posts.sub_body[~df_posts.sub_body.isin(['[removed]','[deleted]'])]\n",
    "    df_posts['text'] = df_posts.sub_title + \" \" + df_posts['sub_body'].fillna(\"\")\n",
    "    \n",
    "    # For comments data, remove rows that are [removed], [deleted], NaN\n",
    "    df_comments = df_comments[~df_comments.com_body.isin(['[removed]','[deleted]'])]\n",
    "    df_comments = df_comments[~df_comments.com_body.isnull()]\n",
    "    \n",
    "    # Rename columns \n",
    "    df_posts.rename(columns = {'sub_date':'date'}, inplace = True)\n",
    "    df_comments.rename(columns = {'com_body':'text', 'com_date':'date'}, inplace = True)\n",
    "    \n",
    "    # Flatten data and order by date\n",
    "    df_preprocessed = pd.concat([df_posts[['sub_id', 'text', 'date']], df_comments[['sub_id', 'text', 'date']]])\n",
    "    df_preprocessed = df_preprocessed.sort_values(by='date', ascending=True)\n",
    "    \n",
    "    df_preprocessed['date'] = pd.to_datetime(df_preprocessed['date'])\n",
    "    \n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\beth\\b', 'ethereum', case = False)\n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\betc\\b', 'ethereum', case = False)\n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\blit\\b', 'litecoin', case = False)\n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\bltc\\b', 'litecoin', case = False)\n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\bbtc\\b', 'bitcoin', case = False)\n",
    "    df_preprocessed['text'] = df_preprocessed['text'].str.replace(r'\\bdoge\\b', 'dogecoin', case = False)\n",
    "    \n",
    "    return df_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Split document based on time frame intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df):\n",
    "    start_date = np.datetime64(df.date.min(), 'D')\n",
    "    end_date = np.datetime64(df.date.max(), 'D')\n",
    "    time_span = (start_date, end_date)\n",
    "    return time_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timeframes(time_span, delta):\n",
    "    # Calculate number of time frames\n",
    "    start_date, end_date = time_span\n",
    "    if ((end_date - start_date)%delta == 0):\n",
    "        N = int((end_date+1 - start_date) / delta)\n",
    "    else:\n",
    "        N = int((end_date+1 - start_date) / delta+1)\n",
    "        \n",
    "    # Get time-frame intervals then store them in start_date_list and end_date_list\n",
    "    start_date_list = np.arange(start_date, end_date+1, delta, dtype='datetime64[D]')\n",
    "    end_date_list = np.arange(start_date+delta, end_date+1, delta, dtype='datetime64[D]')\n",
    "    if end_date_list[-1] < end_date+1:\n",
    "        end_date_list = np.append(end_date_list, end_date+1)\n",
    "    \n",
    "    timeframe_list = [start_date_list, end_date_list]\n",
    "    \n",
    "    return N, timeframe_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_timeframes_str(timeframe_list):\n",
    "    start_date_list = timeframe_list[0] \n",
    "    end_date_list = timeframe_list[1]\n",
    "    # Format timeframe intervals    \n",
    "    formatted_timeframes_str = ['[ ' + str(start_date) + ' - ' + str(end_date) + ' ) 'for start_date, end_date in zip(start_date_list, end_date_list)]\n",
    "    return formatted_timeframes_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_by_timeframe(timeframe_list, df):\n",
    "    start_date_list = timeframe_list[0] \n",
    "    end_date_list = timeframe_list[1]\n",
    "    N = len(start_date_list)\n",
    "    \n",
    "    # Split document into list of time-frame documents (doc_list[i]) \n",
    "    doc_list = [None] * N\n",
    "    \n",
    "    for i in range(N):\n",
    "        start_date = str(start_date_list[i])\n",
    "        end_date = str(end_date_list[i])\n",
    "        mask = (start_date <= df['date']) & (df['date'] < end_date)\n",
    "        df_doc = df.loc[mask]\n",
    "        doc_list[i] = df_doc['text'].to_list()\n",
    "         \n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Process words (tokenisation, building N-grams and lemmatisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stopwords from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['way', 'thank', 'address', 'lol', 'lot', 'people', 'info', 'year', 'sub', 'term', 'bread', 'file', 'question', 'word',\n",
    "                   'think', 'use', 'say', 'fuck', 'come', 'understand', 'try', 'see', 'send', 'look', 'want', 'disagree', 'celsius', 'need',\n",
    "                   'make', 'write', 'know', 'let', 'help', 'block',\n",
    "                   'always', 'read', 'thing', 'hope', 'well', 'work', 'happen', 'congratulation', 'team', 'bag', 'complete', 'even', \n",
    "                   'full', 'happy', 'tip', 'color', 'sweet', 'find', 'talk', 'play', 'step', 'create', 'new', 'different', 'mention', 'name',\n",
    "                   'aware', 'automatically', 'gotcha', 'check', 'generate', 'shoe', 'part', 'coffee', 'test', 'forget', 'yet', 'ever', 'shitty',\n",
    "                   'fucking', 'really', 'right', 'still', 'much', 'never', 'also'])\n",
    "\n",
    "def process_words(texts, stop_words=stop_words, allowed_tags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \n",
    "    \"\"\"Convert a document into a list of lowercase tokens, build bigrams-trigrams, implement lemmatisation\"\"\"\n",
    "    \n",
    "    # remove urls\n",
    "    for i in range(len(texts)):\n",
    "        texts[i] = re.sub(r'http\\S+', '', texts[i])\n",
    "        \n",
    "    # build bigrams and trigrams models\n",
    "    bigram = gensim.models.Phrases(texts, min_count=20, threshold=100)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    \n",
    "    # remove stopwords, short tokens and letter accents \n",
    "    texts = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    # bi-gram and tri-gram implementation\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    \n",
    "    # Using spacy 'en_core_web_sm'model with only tagger\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "    # implement lemmatisation and filter out unwanted part of speech tags\n",
    "    texts_out = []\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_tags])\n",
    "    \n",
    "    # remove stopwords and short tokens again after lemmatisation\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc), deacc=True, min_len=3) if word not in stop_words] for doc in texts_out]    \n",
    "    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_processed(formatted_timeframes_str, numOfSamples_list, doc_list_ready):\n",
    "    df_processed = pd.DataFrame({'Timeframe interval': formatted_timeframes_str,\n",
    "                                  'Sample size': numOfSamples_list,\n",
    "                                  'Document': doc_list_ready})\n",
    "\n",
    "    df_processed['Sample size']= df_processed['Sample size'].astype(int)\n",
    "    df_processed.index = ['Timeframe ' + str(i+1) for i in range(len(doc_list_ready))]\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • Create the Dictionary and Corpus for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bagOfWords(doc_list_ready):\n",
    "    N = len(doc_list_ready)\n",
    "    # Create Dictionary\n",
    "    id2word_list = [None] * N\n",
    "    for i in range(N):\n",
    "        id2word_list[i] = corpora.Dictionary(doc_list_ready[i])\n",
    "\n",
    "    # Create Corpus: Term Document Frequency\n",
    "    corpus_list = [None] * N\n",
    "    for i in range(N):\n",
    "        corpus_list[i] = [id2word_list[i].doc2bow(text) for text in doc_list_ready[i]]\n",
    "    return id2word_list, corpus_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_with_lda_models(corpus_list, id2word_list, num_topics_list, random_state = 100):\n",
    "    num_of_timeframes = len(corpus_list)\n",
    "    topics_list = [None] * num_of_timeframes\n",
    "    \n",
    "    for i in range(num_of_timeframes):\n",
    "        if corpus_list[i] == []:\n",
    "            topics_list[i] = []\n",
    "            continue\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus_list[i],\n",
    "                                                       id2word=id2word_list[i],\n",
    "                                                       num_topics=num_topics_list[i], \n",
    "                                                       random_state=random_state,\n",
    "                                                       chunksize=100,\n",
    "                                                       passes=10,\n",
    "                                                       per_word_topics=True,\n",
    "                                                       workers = 8)\n",
    "        \n",
    "        topics_scores_words = lda_model.show_topics(formatted=False)\n",
    "        topics_words = [([word[0] for word in tp[1]]) for tp in topics_scores_words]\n",
    "        topics = [None] * num_topics_list[i]\n",
    "        for j in range(num_topics_list[i]):\n",
    "            # Label each topic with the highest weightage word \n",
    "            topics[j] = topics_words[j][0]\n",
    "        topics_list[i] = np.unique(topics)\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_models(corpus_list, id2word_list, num_topics_list, random_state = 100):\n",
    "    num_of_timeframes = len(corpus_list)\n",
    "    lda_model_list = [None] * num_of_timeframes\n",
    "\n",
    "    for i in range(num_of_timeframes):\n",
    "        if corpus_list[i] == []:\n",
    "            continue\n",
    "        lda_model_list[i] = gensim.models.LdaMulticore(corpus=corpus_list[i],\n",
    "                                                       id2word=id2word_list[i],\n",
    "                                                       num_topics=num_topics_list[i], \n",
    "                                                       random_state=random_state,\n",
    "                                                       chunksize=100,\n",
    "                                                       passes=10,\n",
    "                                                       per_word_topics=True,\n",
    "                                                       workers = 8)\n",
    "    return lda_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(lda_model_list, num_topics_list):\n",
    "    num_of_timeframes = len(lda_model_list)\n",
    "    topics_list = [None] * num_of_timeframes\n",
    "    for i in range(num_of_timeframes):\n",
    "        if lda_model_list[i] == None:\n",
    "            topics_list[i] = []\n",
    "        else:\n",
    "            topics_scores_words = lda_model_list[i].show_topics(formatted=False)\n",
    "            topics_words = [([word[0] for word in tp[1]]) for tp in topics_scores_words]\n",
    "            topics = [None] * num_topics_list[i]\n",
    "            for j in range(num_topics_list[i]):\n",
    "                # Label each topic with the highest weightage word \n",
    "                topics[j] = topics_words[j][0]\n",
    "            topics_list[i] = np.unique(topics)\n",
    "    return topics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_topics(topics_list):\n",
    "    max_len = max([len(i) for i in topics_list])\n",
    "    df_topics = pd.DataFrame(topics_list)\n",
    "    df_topics.index = ['Timeframe ' + str(i+1) for i in range(len(topics_list))]\n",
    "    df_topics.columns = ['Topic ' + str(i+1) for i in range(max_len)]\n",
    "    df_topics.fillna('-', inplace = True)\n",
    "    return df_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_pair_list(topics_list):\n",
    "    node_pair_list = []\n",
    "    for topics in topics_list:\n",
    "        node_pair = list(permutations(topics, 2))\n",
    "        node_pair_list += node_pair\n",
    "    return node_pair_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_graph(topics_list):\n",
    "    G = nx.Graph()\n",
    "    nodes = list(set().union(*topics_list))\n",
    "    G.add_nodes_from(nodes)\n",
    "    node_pair_list = []\n",
    "    for topics in topics_list:\n",
    "        node_pair = list(combinations(topics, 2))\n",
    "        G.add_edges_from(node_pair)\n",
    "        node_pair_list += node_pair\n",
    "    ebc = nx.edge_betweenness_centrality(G, normalized=False)\n",
    "    nx.set_edge_attributes(G, ebc, 'betweenness')\n",
    "    return G, node_pair_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(filename, mylist):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(mylist, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_list(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_dict(l):\n",
    "    first2second = OrderedDict()\n",
    "    for first, second in l:\n",
    "        first2second.setdefault(first, []).append(second)\n",
    "    result = [tuple(v) for v in first2second.values()]\n",
    "    values = [sum(x) for x in result]\n",
    "    keys = first2second.keys()\n",
    "    d = dict(zip(keys, values))\n",
    "    ranked_dict = dict(sorted(d.items(), key=lambda item: item[1], reverse = True))\n",
    "    return ranked_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
